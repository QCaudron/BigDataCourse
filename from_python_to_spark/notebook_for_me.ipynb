{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Python to Spark (PySpark)\n",
    "\n",
    "PySpark is a Python API for Apache Spark\n",
    "\n",
    "Getting started with Spark for Python programmers is particularly easy if one is using a certain programming style: \n",
    "maps, filters and lambda functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda functions in Python\n",
    "\n",
    "Python supports the creation of anonymous functions (i.e. functions that are not bound to a name) at runtime, using a construct called \"lambda\".\n",
    "\n",
    "Sometimes you need to pass a function as an argument, or you want to do a short but complex operation multiple times. You could define your function the normal way, or you could make a lambda function, a mini-function that returns the result of a single expression. The two definitions are completely identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##traditional named function\n",
    "def add(a,b): return a+b\n",
    "\n",
    "##lambda function\n",
    "add2 = lambda a,b: a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the lambda function is that it is in itself an expression, and can be used inside another statement. Here's an example using the map function, which calls a function on every element in a list, and returns a list of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "squares = map(lambda a: a*a, [1,2,3,4,5])\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: mapping the list in Python\n",
    "\n",
    "To get started with the exercise:\n",
    "\n",
    "```bash\n",
    "cd from_python_to_spark/\n",
    "```\n",
    "and inspect the exercise1.py file therein.\n",
    "\n",
    "Suppose you need to perform a transformation on a list of element. For instance, to calculate a square of each element of the list. One way to write this in Python would be as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of squares:  [1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = []\n",
    "for number in numbers:\n",
    "    squares.append(number*number)\n",
    "    # Now, squares should have [1,4,9,16,25]\n",
    "print \"List of squares: \", squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonic way\n",
    "\n",
    "Python provides a few ways to re-write the same piece of code in a more compact form: list comprehensions and with the map. Python programmers who do not use Spark typically prefer the list comprehensions to the map. But using the map is what allows you to adjust to Spark way of programming the easiest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of squares:  [1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = map(lambda x: x*x, numbers)\n",
    "#Now, squares should have [1,4,9,16,25]\n",
    "print \"List of squares: \", squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: filtering the list in Python\n",
    "\n",
    "What if you're more interested in filtering the list? Say you want to remove every element with a value equal to or greater than 4? (Okay, so the examples aren't very realistic. Whatever...) A Python neophyte might write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers under 4 only:  [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "numbers_under_4 = []\n",
    "for number in numbers:\n",
    "    if number < 4:\n",
    "        numbers_under_4.append(number)\n",
    "        # Now, numbers_under_4 contains [1,4,9]\n",
    "print \"Numbers under 4 only: \",numbers_under_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could reduce the size of the code with the filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers under 4 only:  [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "numbers_under_4 = filter(lambda x: x < 4, numbers)\n",
    "# Now, numbers_under_4 contains [1,2,3]\n",
    "print \"Numbers under 4 only: \",numbers_under_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Pandas data structures and functionality\n",
    "\n",
    "Pandas is Python's answer to R.  It's a good tool for small(ish) data analysis -- i.e., when everything fits into memory. The basic new \"noun\" in pandas is the **data frame**. As a part of pre-exercises, you have received an iPython notebook with some Pandas case study. \n",
    "\n",
    "It's like a table, with rows and columns (e.g., as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. (Like in SQL, columns are homogeneous.)\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)\n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e., SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from other column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation**: If you're an Excel cognosceti, you may appreciate this.\n",
    "  - **NA handling**: Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "  \n",
    "## Map and filter in Pandas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Census_Tract_Number</th>\n",
       "      <th>NUM_ALL</th>\n",
       "      <th>NUM_FHA</th>\n",
       "      <th>PCT_NUM_FHA</th>\n",
       "      <th>AMT_ALL</th>\n",
       "      <th>AMT_FHA</th>\n",
       "      <th>PCT_AMT_FHA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>49</td>\n",
       "      <td>103.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>113</td>\n",
       "      <td>603.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>105</td>\n",
       "      <td>124.04</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State_Code  County_Code  Census_Tract_Number  NUM_ALL  NUM_FHA  \\\n",
       "0           8           75                  NaN        1        1   \n",
       "1          28           49               103.01        1        1   \n",
       "2          40            3                  NaN        1        1   \n",
       "3          39          113               603.00        3        3   \n",
       "4          12          105               124.04        2        2   \n",
       "\n",
       "   PCT_NUM_FHA  AMT_ALL  AMT_FHA  PCT_AMT_FHA  \n",
       "0          100      258      258          100  \n",
       "1          100       71       71          100  \n",
       "2          100      215      215          100  \n",
       "3          100      206      206          100  \n",
       "4          100      303      303          100  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('../preexercise/data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9\n",
       "1    29\n",
       "2    41\n",
       "3    40\n",
       "4    13\n",
       "Name: State_Code2, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"State_Code2\"] = df[\"State_Code\"].apply(lambda x: x+1)\n",
    "\n",
    "df[\"State_Code2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Census_Tract_Number</th>\n",
       "      <th>NUM_ALL</th>\n",
       "      <th>NUM_FHA</th>\n",
       "      <th>PCT_NUM_FHA</th>\n",
       "      <th>AMT_ALL</th>\n",
       "      <th>AMT_FHA</th>\n",
       "      <th>PCT_AMT_FHA</th>\n",
       "      <th>State_Code2</th>\n",
       "      <th>County_Code2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>113</td>\n",
       "      <td>603.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>105</td>\n",
       "      <td>124.04</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>86</td>\n",
       "      <td>9808.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>103</td>\n",
       "      <td>207.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>119</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>100</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State_Code  County_Code  Census_Tract_Number  NUM_ALL  NUM_FHA  \\\n",
       "3          39          113               603.00        3        3   \n",
       "4          12          105               124.04        2        2   \n",
       "5          12           86              9808.00        1        1   \n",
       "7          12          103               207.00        2        2   \n",
       "8          36          119                30.00        1        1   \n",
       "\n",
       "   PCT_NUM_FHA  AMT_ALL  AMT_FHA  PCT_AMT_FHA  State_Code2 County_Code2  \n",
       "3          100      206      206          100           40           39  \n",
       "4          100      303      303          100           13           12  \n",
       "5          100      188      188          100           13           12  \n",
       "7          100      100      100          100           13           12  \n",
       "8          100      354      354          100           37           36  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['County_Code'] > 75]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: from Python to PySpark\n",
    "\n",
    "Say I want to map and filter a list at the same time. In other words, I'd like to see the square of each element in the list where said element is under 4. Once more, the Python neophyte way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = []\n",
    "for number in numbers:\n",
    "    if number < 4:\n",
    "        squares.append(number*number)\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before re-writing it in PySpark, re-write it using map and filter expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = map(lambda x: x*x, filter(lambda x: x < 4, numbers))\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "#I do not need to create the Spark Context in the notebook, but you do...\n",
    "#sc = SparkContext(\"My First App\")\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "squares_rdd = numbers_rdd.filter(lambda x: x < 4).map(lambda x: x*x)\n",
    "print squares_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Spark jobs via Slurm\n",
    "\n",
    "Before starting with the exercise2.py, you need to make sure the scratch is set up.\n",
    "Look for your scratch folder:\n",
    "\n",
    "```bash\n",
    "ls -l /scratch/network/<your_username>\n",
    "```\n",
    "\n",
    "create it if necessary:\n",
    "```bash\n",
    "mkdir /scratch/network/<your_username>\n",
    "```\n",
    "\n",
    "Define an environmental variable to store its location:\n",
    "\n",
    "```bash\n",
    "export SCRATCH_PATH=\"/scratch/network/<your_username>\"\n",
    "``` \n",
    "\n",
    "The Slurm submission file for Spark job will look like:\n",
    "\n",
    "```bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -t 00:05:00\n",
    "#SBATCH --ntasks-per-node 2\n",
    "#SBATCH --cpus-per-task 3\n",
    "\n",
    "module load spark/hadoop2.6/1.4.1\n",
    "spark-start\n",
    "echo $MASTER\n",
    "\n",
    "spark-submit --total-executor-cores 2 exercise2.py\n",
    "```\n",
    "\n",
    "Monitor the progress of your Spark application:\n",
    "\n",
    "```bash\n",
    "squeue -u alexeys\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "            219838       all slurm_fo  alexeys  R       0:04      1 adroit-06\n",
    "```             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations in Python and PySpark (quick look forward)\n",
    "\n",
    "It is Pythonic to operate on lists - elementwise operations (maps), filtering, etc. In many other languages, starting with Lisp but extending to many \"functional\" programming languages, a different style is preferred:\n",
    "\n",
    "The idea is that if `f` is a function, then one thinks of the application\n",
    ">          \n",
    "    list   |---->   [ f(x) for x in list ]\n",
    "\n",
    "on lists as a function of _two_ arguments: `f` and `list`.  The idea of viewing the function `f` as a parameter is typical in functional programming languages, and can be taken as a definition of the later term.\n",
    "\n",
    "Some common idioms in this style, with Pythonic equivalents, are:\n",
    "\n",
    "- `map(f, list) === [ f(x) for x in list ]`: Apply `f` element-wise to `list`.\n",
    "- `filter(f, list) === [ x for x in list if f(x) ]`: Filter `list` using `f`.\n",
    "- `flatMap(f, list) === [ f(x) for y in list for x in y ]`: Here `f` is a function that eats elements (of the type contained in list) and spits out lists, and `flatMap` first applies f element-wise to the elements of `list` and then _flattens_ or _concatenates_ the resulting lists.  It is sometimes also called `concatMap`.\n",
    "- `reduce(f, list[, initial])`: Here `f` is a function of _two_ variables, and folds over the list applying `f` to the \"accumulator\" and the next value in the list.  That is, it performs the following recursion\n",
    "\n",
    "$$    a_{-1} = \\mathrm{initial} $$\n",
    "$$    a_i = f(a_{i-1}, \\mathrm{list}_i) $$\n",
    "\n",
    "with the with the final answer being $a_{\\mathrm{len}(\\mathrm{list})-1}$.  (If initial is omitted, just start with $a_0 = \\mathrm{list}_0$.)  For instance,\n",
    ">           \n",
    "    reduce(lambda x,y: x+y, [1,2,3,4]) = ((1+2)+3)+4 = 10\n",
    "    \n",
    "    \n",
    "### Remark:\n",
    "This is where the name \"map reduce\" comes from.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark core fundamentals\n",
    "(slides)\n",
    "\n",
    "# Anatomy of the Spark application\n",
    "\n",
    "\n",
    "# Spark transformations and actions\n",
    "\n",
    "\n",
    "# Working with key-value pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading data into RDD\n",
    "\n",
    "To start the exercise, change into loading_data folder:\n",
    "```bash\n",
    "cd loading_data\n",
    "```\n",
    "\n",
    "In the root folder you will find a set of files starting with `load`, and a few folders. \n",
    "Let us inspect the load1_unstructured.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'', u'', u'', u'', u'                        THE ADVENTURES OF SHERLOCK HOLMES', u'', u'                               Arthur Conan Doyle', u'', u'', u'']\n",
      "Input dataset has  53271  lines\n",
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "[(u'the', 22635), (u'of', 11167), (u'and', 11086), (u'to', 10707), (u'a', 10433), (u'I', 10183), (u'in', 7006), (u'that', 6911), (u'was', 6779), (u'his', 4955)]\n",
      "Elapsed time:  1.72066187859\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def main1(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    #By default it assumes file located on hdfs folder, \n",
    "    #but by prefixing \"file://\" it will search the local file system\n",
    "    #Can specify a folder, can pass list of folders or use wild character\n",
    "    input_rdd = sc.textFile(\"../loading_data/unstructured/\")\n",
    "\n",
    "    #inspect it, understand how it is structured (list of strings-lines)\n",
    "    print input_rdd.take(10)\n",
    "    print \"Input dataset has \", input_rdd.count(), \" lines\"\n",
    "\n",
    "    counts = input_rdd.flatMap(lambda line: line.split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "\n",
    "def main2(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    #Use alternative approach: load the dinitial file into a pair RDD\n",
    "    input_pair_rdd = sc.wholeTextFiles(\"../loading_data/unstructured/\")\n",
    "\n",
    "    #inspect it, understand how it is structured (list of strings-lines)\n",
    "    print input_pair_rdd.take(3)\n",
    "    print \"Input dataset has \", input_pair_rdd.count(), \" files\"\n",
    "    counts = input_pair_rdd.flatMap(lambda line: line[1].split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Try the record-per-line-input\n",
    "    main1(sys.argv)\n",
    "    #Use alternative approach: load the initial file into a pair RDD\n",
    "    #main2(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV\n",
    "\n",
    "Next, we are going to learn how to load data in structured CSV format. There is at least two ways to do that:\n",
    "\n",
    "1) Read the files line by line with textFiles() method, split on delimiter\n",
    "\n",
    "Similarly to Python, there is a data structured designed to be used when working with structured data (I mean Pandas Dataframes), it is also called the dataframe (a concept closely linked to Spark SQL). There is a way to read CSV directly into Spark dataframe \n",
    "\n",
    "2) Read the files into dataframe using spark-csv module from Databricks\n",
    "https://github.com/databricks/spark-csv\n",
    "\n",
    "You do not need to install it, I did the work for you by adding the build Jars into the appropriate /lib folder...\n",
    "\n",
    "### Load CSV\n",
    "\n",
    "### Mini-exercise on loading CSV\n",
    "\n",
    "Use what you have learned in the load2_csv.py exercise to load a set of CSV datasets:\n",
    "\n",
    "-- Actor\n",
    "\n",
    "-- Movie\n",
    "\n",
    "-- Actor playing in movie (relationships)\n",
    "\n",
    "and find movies where **Tom Hanks** played in.\n",
    "\n",
    "Save the answer in the JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "\n",
    "Let us consider a very simple machine learning example of logistic regression.\n",
    "Logistic regression is an iterative machine learning algorithm that seeks to find the best hyperplane that separates two sets of points in a multi-dimensional feature space. It can be used to classify messages into spam vs non-spam, for example. Because the algorithm applies the same MapReduce operation repeatedly to the same dataset, it benefits greatly from caching the input in RAM across iterations.\n",
    "\n",
    "## Non-MLlib implementation\n",
    "\n",
    "First, let us consider the non-MLlib implementation and try to evaluate the effect of caching and partitioning on the perfromance. We're going to try to learn the rule that y(x) = 1 if x < fraction_positive, 0 otherwise\n",
    "\n",
    "Our training sample will be generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = 10**3\n",
    "fraction_positive = 0.5\n",
    "\n",
    "def y(x):\n",
    "    return 1 if x < fraction_positive else 0\n",
    "\n",
    "def generate_sample():\n",
    "    sample_X = np.arange(0, 1, 1.0/N)\n",
    "    np.random.shuffle( sample_X) # In-place shuffle!\n",
    "    sample_Y = map(y, sample_X)\n",
    "    return (sample_X, sample_Y)\n",
    "\n",
    "(sample_X, sample_Y) = generate_sample()\n",
    "\n",
    "\n",
    "## By hand.  This is the example code taken from the Spark Examples on the website.\n",
    "#  This is much slower than the above code, so I'm not going to even run it (or extract predictions, or test it..)\n",
    "start = time.time()\n",
    "def logistic_by_hand(ITERATIONS,nparts):\n",
    "    points = ( sc.parallelize( zip(sample_X, sample_Y), nparts)\n",
    "                 .map(lambda (x,y): LabeledPoint(y, [1, x]))\n",
    "                 .cache() )\n",
    "    w = np.random.ranf(size = 2) # current separating plane\n",
    "    print \"Original random plane: %s\" % w\n",
    "    for i in xrange(ITERATIONS):\n",
    "        gradient = points.map(\n",
    "            lambda pt: (1 / (1 + np.exp(-pt.label*(w.dot(pt.features)))) - 1) * pt.label * pt.features\n",
    "        ).reduce(lambda a, b: a + b)\n",
    "        w -= gradient\n",
    "    print \"Final separating plane: %s\" % w\n",
    "\n",
    "logistic_by_hand(20,3)\n",
    "end = time.time()\n",
    "print \"Elapsed time: \", (end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib based implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.896\n",
      "Elapsed time:  15.8722097874\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "## Using MLLib and it's data structures.  This is fairly quick.\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "points = ( sc.parallelize( zip(sample_X, sample_Y), 3)\n",
    "             .map(lambda (x,y): LabeledPoint(y, [1, x]))\n",
    "             .cache() )\n",
    "model = LogisticRegressionWithSGD.train(points)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = points.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(points.count())\n",
    "print \"Accuracy on training set: %s\" % (1 - trainErr)\n",
    "end = time.time()\n",
    "print \"Elapsed time: \", (end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Play with the `fraction_positive` parameter: What happens to the accuracy measure as `fraction_positive` gets below 0.30 or above 0.70? (You should be somewhat disappointed with the results!)  What do you think is happening, and can you improve on it?\n",
    "1. Play with the \"by hand\" version (.. after lowering N to say 10**4 or so): Figure out what it's actually doing and how to use it to get results.  How much slower than the MLLib version does it seem to be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project1: web mining and text processing \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Spark applications with Scala API\n",
    "\n",
    "Apache Spark is written in Scala. Scala (along with Python and Java) is among three languages supported by Spark, and in fact Scala functionality is typically added the first to new Spark releases.\n",
    "\n",
    "\n",
    "## Preparing our first Scala Spark application\n",
    "\n",
    "Let us start with an PySpark application we have prepared on one of the previous steps. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "[(u'the', 22635), (u'of', 11167), (u'and', 11086), (u'to', 10707), (u'a', 10433), (u'I', 10183), (u'in', 7006), (u'that', 6911), (u'was', 6779), (u'his', 4955)]\n",
      "Elapsed time:  1.07145404816\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def main1(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    input_rdd = sc.textFile(\"../loading_data/unstructured/\",10)\n",
    "    counts = input_rdd.flatMap(lambda line: line.split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main1(sys.argv)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The same application can be re-written in Scala as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.SparkContext._\n",
    "\n",
    "object WordCount {\n",
    "  def main(args: Array[String]) {\n",
    "\n",
    "    val conf = new SparkConf().setAppName(\"WordCount\")\n",
    "\n",
    "\n",
    "    val textFile = spark.textFile(\"../loading_data/unstructured/\",10)\n",
    "    val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "                 .map(word => (word, 1))\n",
    "                 .reduceByKey(_ + _)\n",
    "    println(\"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\")\n",
    "    println(counts.takeOrdered(10).(Ordering[Int].reverse.on(x=>x._2)))\n",
    "        \n",
    "    val t1 = System.nanoTime()\n",
    "    println(\"Elapsed time: \" + (t1 - t0)/1000000000.)\n",
    "    spark.stop()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Scala Spark application Q/A\n",
    "\n",
    "Q: So you've written some Spark code in Scala. How do you submit it to Spark and run it?  \n",
    "A: Use `sbt` or `maven` to package it into a Java jar, and submit it to Spark using `spark-submit`\n",
    "\n",
    "Q: What's a Java jar?  \n",
    "A: JAR (Java Archive) is a package file format typically used to aggregate many Java class files and associated metadata and resources (text, images, etc.) into one file to distribute application software or libraries on the Java platform.\n",
    "\n",
    "### Packaging with `sbt`\n",
    "\n",
    "**What is SBT?**  \n",
    "SBT is a modern build tool written in/for Scala, though it is also a general purpose build tool  \n",
    "\n",
    "**Why SBT?**\n",
    "- Good dependency management\n",
    "- Full Scala language support for creating tasks\n",
    "- Launch REPL in project context\n",
    "\n",
    "Create a root directory for your project and run:\n",
    "```bash\n",
    "mkdir -p src/{main,test}/{resources,scala}\n",
    "mkdir lib project\n",
    "```\n",
    "within it. \n",
    "\n",
    "This script will automatically create the proper `sbt` directory structure, which borrows from the Java `maven` directory structure. The script will also generate a template `build.sbt` file at the top of the directory that you should fill out with the appropriate versions and dependencies for your app.\n",
    "\n",
    "Then we can take our Scala code, and put it in the src folder (you should have it in the main folder, so just move it there):\n",
    "\n",
    "```bash\n",
    "mv WordCount.scala src/main/scala/\n",
    "```\n",
    "\n",
    "**Project Layout (Directory structure)**   \n",
    "\n",
    "`project` – project definition files  \n",
    "`project/build/` *yourproject* `.scala` – the main project definition file  \n",
    "`project/build.properties` – project, sbt and scala version definitions  \n",
    "`src/main` – your app code goes here, in a subdirectory indicating the code’s language (e.g. src/main/scala, src/main/java)  \n",
    "`src/main/resources` – static files you want added to your jar (e.g. logging config)  \n",
    "`src/test` – like src/main, but for tests  \n",
    "`lib_managed` – the jar files your project depends on. Populated by sbt update  \n",
    "`target` – the destination for generated stuff (e.g. generated thrift\n",
    "code, class files, jars)  \n",
    "\n",
    "#### `build.sbt`: Dependencies and versioning\n",
    "\n",
    "Example `simple.sbt` (located in the root directory of your project) \n",
    "\n",
    "```scala\n",
    "name := \"WordCount\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.10.4\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "    // Spark dependency\n",
    "    \"org.apache.spark\" % \"spark-core_2.10\" % \"1.4.1\" % \"provided\"\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "#### Assembly.sbt to build a fat Jar\n",
    "\n",
    "Example assembly.sbt located in the /project folder of your project:\n",
    "\n",
    "```scala\n",
    "resolvers += Resolver.url(\"artifactory\", url(\"http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases\"))(Resolver.ivyStylePatterns)\n",
    "\n",
    "addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n",
    "```\n",
    "\n",
    "### Running (submitting a `jar` to Spark)\n",
    "\n",
    "1. Run `sbt assembly` in your project's home directory. The output to console will tell you the name and location of the resulting jar (under `./target`) \n",
    "\n",
    "You should now see the Jar file generated:\n",
    "```bash\n",
    "[alexeys@bd scala_spark2] ll target/scala-2.10/\n",
    "total 6968\n",
    "drwxr-xr-x. 2 alexeys cses    4096 Dec  9 10:43 classes\n",
    "-rw-r--r--. 1 alexeys cses 7129172 Dec  9 10:43 WordCount-assembly-1.0.jar\n",
    "```\n",
    "\n",
    "2. In the Slurm batch script, use spark-submit as usual to submit the Spark app, but you would need to specify the --class and the path to jar from the current folder, for instance:\n",
    "\n",
    "```bash\n",
    "spark-submit \\\n",
    "  --class \"WordCount\" \\\n",
    "  --master local[*] \\\n",
    "  target/scala-2.10/WordCount-assembly-1.0.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
